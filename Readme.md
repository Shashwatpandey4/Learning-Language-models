# Learning Language Models

1. Word Embeddings (Word2Vec, GloVe)
2. RNN, LSTM, GRUs
3. Implement a simple LSTM model

4. Attention is all you need (Read & Understand)
5. Implement self-attention from scratch
6. Implement Transformer Encoder-Decoder Model (in pytorch)

7. Masked Language Models (MLM)
8. Train BERT 
9. Fine-tune BERT 

10. Causal attention and auto regressive modeling
11. Implement GPT (Karpathy's video)
12. Implement & Train GPT-2

13. T5 (seq2seq), BART(denoising), & ALBERT(eff. BERT)
14. Fine Tune T5 for text generation
15. DIstillBERT & TinyBERT

16. LLM training strategies
